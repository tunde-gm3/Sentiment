{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Sentiments (Part 4)\n",
    "\n",
    "This section checks out the performance of the analysis when Stemming and Lemmatization were used to determine the roots of each word. The idea here is that word with similar root words may boost the probability of prediction for each class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9020\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\9020\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import random\n",
    "random.seed (1)\n",
    "\n",
    "train = pd.read_csv('Train_Data.csv')\n",
    "test = pd.read_csv('Test_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiating Stemmer\n",
    "s_stemmer = SnowballStemmer(language = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_stem(sente):\n",
    "    '''\n",
    "    Function used to change each word to the root form through stemming\n",
    "    Stop words are not used in this process\n",
    "    '''\n",
    "    out = []\n",
    "    sente_li = sente.split(' ')\n",
    "    for word in sente_li:\n",
    "        if word not in nlp.Defaults.stop_words:\n",
    "            out.append(s_stemmer.stem(word))\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_lemma(senten):\n",
    "    '''\n",
    "    Function used to change each word to the root form through lemmatization\n",
    "    Stop words are not used in this process    \n",
    "    '''\n",
    "    out = []\n",
    "    sente = nlp(senten)\n",
    "    for word in sente:\n",
    "        if word not in nlp.Defaults.stop_words:\n",
    "            out.append(word.lemma_)\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone  \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  7921   \n",
       "1  7922   \n",
       "\n",
       "                                                                                                                 tweet  \n",
       "0                                        I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks  \n",
       "1  currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['tweet_stem'] = train['tweet'].apply(do_stem)\n",
    "test['tweet_stem'] = test['tweet']\n",
    "\n",
    "train['tweet_lemma'] = train['tweet'].apply(do_lemma)\n",
    "test['tweet_lemma'] = test['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_stem</th>\n",
       "      <th>tweet_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "      <td>#fingerprint #pregnanc test https://goo.gl/h1mfqv #android #app #beauti #cute #health #iger #iphoneon #iphonesia #iphon</td>\n",
       "      <td># fingerprint # pregnancy Test https://goo.gl/h1MfQV # android # app # beautiful # cute # health # iger # iphoneonly # iphonesia # iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "      <td>final transpar silicon case ^^ thank uncl :) #yay #soni #xperia #s #sonyexperias… http://instagram.com/p/yget5jc6jm/</td>\n",
       "      <td>finally a transparant silicon case ^^ thank to -PRON- uncle :) # yay # Sony # Xperia # s # sonyexperias … http://instagram.com/p/yget5jc6jm/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "\n",
       "                                                                                                                                 tweet  \\\n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone   \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/   \n",
       "\n",
       "                                                                                                                tweet_stem  \\\n",
       "0  #fingerprint #pregnanc test https://goo.gl/h1mfqv #android #app #beauti #cute #health #iger #iphoneon #iphonesia #iphon   \n",
       "1     final transpar silicon case ^^ thank uncl :) #yay #soni #xperia #s #sonyexperias… http://instagram.com/p/yget5jc6jm/   \n",
       "\n",
       "                                                                                                                                    tweet_lemma  \n",
       "0     # fingerprint # pregnancy Test https://goo.gl/h1MfQV # android # app # beautiful # cute # health # iger # iphoneonly # iphonesia # iphone  \n",
       "1  finally a transparant silicon case ^^ thank to -PRON- uncle :) # yay # Sony # Xperia # s # sonyexperias … http://instagram.com/p/yget5jc6jm/  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_stem</th>\n",
       "      <th>tweet_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks</td>\n",
       "      <td>I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks</td>\n",
       "      <td>I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/</td>\n",
       "      <td>currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/</td>\n",
       "      <td>currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  7921   \n",
       "1  7922   \n",
       "\n",
       "                                                                                                                 tweet  \\\n",
       "0                                        I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks   \n",
       "1  currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/   \n",
       "\n",
       "                                                                                                            tweet_stem  \\\n",
       "0                                        I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks   \n",
       "1  currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/   \n",
       "\n",
       "                                                                                                           tweet_lemma  \n",
       "0                                        I hate the new #iphone upgrade. Won't let me download apps. #ugh #apple sucks  \n",
       "1  currently shitting my fucking pants. #apple #iMac #cashmoney #raddest #swagswagswag http://instagr.am/p/UUIS0bIBZo/  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_test(vec, model):\n",
    "    '''\n",
    "    This function is used to tokenize text-data (train.tweet) using countvectorizer {vec}, \n",
    "    and a Machine Learning algorithm {model}, to understand the relationship between the tweet and its label.\n",
    "    It is also used to predict the label of the given test data.\n",
    "    The result of the prediction is now output in a csv file which can be uploaded unto Analytics Vidhya website \n",
    "    to determine its score.\n",
    "    The function also returns the prediction probabilities which can be further combine to see if there could be improvement in \n",
    "    performance.\n",
    "    '''\n",
    "    \n",
    "    print ('Tokenization:\\n ', vec)\n",
    "    print ()\n",
    "    print ('Model:', model)\n",
    "    print ()\n",
    "    X_traindata_dtm = vec.fit_transform(train['tweet'])\n",
    "    model.fit(X_traindata_dtm, train.label)\n",
    "    X_testdata_dtm = vec.transform(test['tweet'])\n",
    "    y_result = model.predict(X_testdata_dtm)\n",
    "     \n",
    "    D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':y_result})\n",
    "    D.to_csv('Result.csv', index=False)\n",
    "    \n",
    "    return model.predict_proba(X_testdata_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_test_lemma(vec, model):\n",
    "    '''\n",
    "    Function used with lemmatization\n",
    "    This function is used to tokenize text-data (train.tweet) using countvectorizer {vec}, \n",
    "    and a Machine Learning algorithm {model}, to understand the relationship between the tweet and its label.\n",
    "    It is also used to predict the label of the given test data.\n",
    "    The result of the prediction is now output in a csv file which can be uploaded unto Analytics Vidhya to determine its score.\n",
    "    The function also returns the prediction probabilities which can be further combine to see if there could be improvement in \n",
    "    performance.\n",
    "    '''\n",
    "    \n",
    "    print ('Tokenization:\\n ', vec)\n",
    "    print ()\n",
    "    print ('Model:', model)\n",
    "    print ()\n",
    "    X_traindata_dtm = vec.fit_transform(train['tweet_lemma'])\n",
    "    model.fit(X_traindata_dtm, train.label)\n",
    "    X_testdata_dtm = vec.transform(test['tweet_lemma'])\n",
    "    y_result = model.predict(X_testdata_dtm)\n",
    "     \n",
    "    D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':y_result})\n",
    "    D.to_csv('Result.csv', index=False)\n",
    "    \n",
    "    return model.predict_proba(X_testdata_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_test_stem(vec, model):\n",
    "    '''\n",
    "    Function used with stemming\n",
    "    This function is used to tokenize text-data (train.tweet) using countvectorizer {vec}, \n",
    "    and a Machine Learning algorithm {model}, to understand the relationship between the tweet and its label.\n",
    "    It is also used to predict the label of the given test data.\n",
    "    The result of the prediction is now output in a csv file which can be uploaded unto Analytics Vidhya to determine its score.\n",
    "    The function also returns the prediction probabilities which can be further combine to see if there could be improvement in \n",
    "    performance.\n",
    "    '''\n",
    "    \n",
    "    print ('Tokenization:\\n ', vec)\n",
    "    print ()\n",
    "    print ('Model:', model)\n",
    "    print ()\n",
    "    X_traindata_dtm = vec.fit_transform(train['tweet_stem'])\n",
    "    model.fit(X_traindata_dtm, train.label)\n",
    "    X_testdata_dtm = vec.transform(test['tweet_stem'])\n",
    "    y_result = model.predict(X_testdata_dtm)\n",
    "    \n",
    "    D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':y_result})\n",
    "    D.to_csv('Result.csv', index=False)\n",
    "    \n",
    "    return model.predict_proba(X_testdata_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "Model: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F1-score = 0.889762413131238\n",
    "model_test_lemma(CountVectorizer(max_df = 0.5, stop_words='english'), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "Model: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F1-score = 0.870341292341096\n",
    "model_test_lemma(CountVectorizer(max_df = 0.5), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "Model: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F1-score = 0.877160712143345\n",
    "model_test_stem(CountVectorizer(max_df = 0.5, stop_words='english'), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "Model: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F1-score = 0.875749865803659\n",
    "model_test_stem(CountVectorizer(max_df = 0.5), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "Model: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probabilities of the test data --> Lemmatization\n",
    "a_lemma = model_test_lemma(CountVectorizer(max_df = 0.5, stop_words='english'), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "Model: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probabilities of the test data --> Stemming\n",
    "a_stem = model_test_stem(CountVectorizer(max_df = 0.5, stop_words='english'), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "Model: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probabilities of the test data --> No Stemming OR Lemmatization\n",
    "a_ = model_test(CountVectorizer(max_df = 0.5, stop_words='english'), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_mean = (a_stem + a_lemma)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_mean = (a_stem + a_lemma + a_)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3421177861827135e-10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mean[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score = 0.895872763551162\n",
    "a_mean_class = [0 if a_mean[i] < 0.5 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':a_mean_class})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score = 0.890699776085467\n",
    "a_mean_class = [0 if a_mean[i] < 0.45 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':a_mean_class})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score = 0.888227350335826\n",
    "a_mean_class = [0 if a_mean[i] < 0.55 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':a_mean_class})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score = 0.895872763551162\n",
    "a_mean_class = [0 if a_mean[i] < 0.49 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':a_mean_class})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score = 0.895872763551162\n",
    "a_mean_class = [0 if a_mean[i] < 0.51 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':a_mean_class})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score = 0.894652734138471\n",
    "a_mean_class = [0 if a_mean[i] < 0.48 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':a_mean_class})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score = 0.892823492211137\n",
    "a_mean_class = [0 if a_mean[i] < 0.52 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':a_mean_class})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score = 0.891766507676512\n",
    "a_mean_class = [0 if all_mean[i] < 0.5 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':a_mean_class})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 1 \n",
    "b = 1\n",
    "c = 0\n",
    "weig_aver_prob = (a*a_ + b*a_lemma + c*a_stem)/(a + b + c)\n",
    "ave_pred = [0 if weig_aver_prob[i] < 0.5 else 1 for i in range(1953)]\n",
    "D = pd.DataFrame({'id':[i for i in range(7921, 9874)], 'label':ave_pred})\n",
    "D.to_csv('Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = 1, b = 1; F1 = 0.897538960600431 {Current best result from combination of two good models}\n",
    "# a = 1, c = 1; F1 = 0.890087917081782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
